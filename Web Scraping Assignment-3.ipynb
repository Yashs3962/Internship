{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "012f92b4",
   "metadata": {},
   "source": [
    "Answer:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "486733ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Results:\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon(product):\n",
    "    url = f'https://www.amazon.in/s?k={product.replace(\" \", \"+\")}'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    product_titles = soup.find_all('span', {'class': 'a-size-medium'})\n",
    "    products = [title.get_text() for title in product_titles]\n",
    "    return products\n",
    "\n",
    "product = \"guitars\"\n",
    "\n",
    "search_results = search_amazon(product)\n",
    "\n",
    "print(\"Search Results:\")\n",
    "for index, result in enumerate(search_results, 1):\n",
    "    print(f\"{index}. {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d61bfcf",
   "metadata": {},
   "source": [
    "Answer:3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b1bcac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\vishs\\anaconda3\\lib\\site-packages (4.20.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\vishs\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\vishs\\anaconda3\\lib\\site-packages (from selenium) (0.25.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\vishs\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\vishs\\anaconda3\\lib\\site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\vishs\\anaconda3\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\vishs\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\vishs\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\vishs\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\vishs\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\vishs\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\vishs\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\vishs\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\vishs\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\vishs\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\vishs\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6ce2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9abb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_images(keyword, num_images):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(\"https://www.google.com/imghp?hl=en\")\n",
    "    search_bar = driver.find_element_by_xpath(\"//input[@name='q']\")\n",
    "    search_bar.send_keys(keyword)\n",
    "    search_bar.send_keys(Keys.ENTER)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff86bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "  for _ in range(3):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907de2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    " images = driver.find_elements_by_xpath(\"//img[@class='rg_i Q4LuWd']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b0d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    " image_urls = []\n",
    "    for img in images[:num_images]:\n",
    "        img_url = img.get_attribute(\"src\")\n",
    "        if img_url:\n",
    "            image_urls.append(img_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66582b4",
   "metadata": {},
   "outputs": [],
   "source": [
    " driver.quit()\n",
    "    return image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee1805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "num_images_per_keyword = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df1a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "for keyword in keywords:\n",
    "    print(f\"Scraping images for keyword: {keyword}\")\n",
    "    image_urls = scrape_images(keyword, num_images_per_keyword)\n",
    "    print(\"Image URLs:\")\n",
    "    for idx, url in enumerate(image_urls, 1):\n",
    "        print(f\"{idx}. {url}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb43339d",
   "metadata": {},
   "source": [
    "Answer:4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4886cfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\vishs\\anaconda3\\lib\\site-packages (4.12.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: requests in c:\\users\\vishs\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\vishs\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\vishs\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vishs\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vishs\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vishs\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vishs\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\vishs\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vishs\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\vishs\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vishs\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4 requests pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02eb90d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_smartphones():\n",
    "    url = \"https://www.flipkart.com/search?q=oneplus+nord\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    smartphones = []\n",
    "\n",
    "    results = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "\n",
    "    for result in results:\n",
    "        details = {}\n",
    "\n",
    "        details['Brand Name'] = result.find('div', {'class': '_4rR01T'}).text\n",
    "        details['Smartphone Name'] = result.find('a', {'class': 'IRpwTa'}).text\n",
    "        details['Colour'] = result.find('div', {'class': '_2WkVRV'}).text\n",
    "        details['RAM'] = result.find('ul', {'class': '_1xgFaf'}).find_all('li')[0].text\n",
    "        details['Storage(ROM)'] = result.find('ul', {'class': '_1xgFaf'}).find_all('li')[1].text\n",
    "        details['Primary Camera'] = result.find('ul', {'class': '_1xgFaf'}).find_all('li')[2].text\n",
    "        details['Secondary Camera'] = result.find('ul', {'class': '_1xgFaf'}).find_all('li')[3].text\n",
    "        details['Display Size'] = result.find('ul', {'class': '_1xgFaf'}).find_all('li')[4].text\n",
    "        details['Battery Capacity'] = result.find('ul', {'class': '_1xgFaf'}).find_all('li')[5].text\n",
    "        details['Price'] = result.find('div', {'class': '_30jeq3 _1_WHN1'}).text\n",
    "        details['Product URL'] = \"https://www.flipkart.com\" + result.find('a', {'class': 'IRpwTa'})['href']\n",
    "\n",
    "        smartphones.append(details)\n",
    "  \n",
    "    return smartphones\n",
    "\n",
    "smartphones = scrape_smartphones()\n",
    "\n",
    "df = pd.DataFrame(smartphones)\n",
    "\n",
    "df.fillna(\"-\", inplace=True)\n",
    "df.to_csv('smartphones.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d144a6",
   "metadata": {},
   "source": [
    "Answer:5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf2470db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates for Alwar not found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_coordinates(city):\n",
    "    search_url = f\"https://www.google.com/maps/search/{city}\"\n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    coordinates_element = soup.find('meta', itemprop='geo')\n",
    "    if coordinates_element:\n",
    "        coordinates_content = coordinates_element['content']\n",
    "        latitude, longitude = coordinates_content.split(',')\n",
    "        return latitude.strip(), longitude.strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "city = \"Alwar\"\n",
    "\n",
    "coordinates = scrape_coordinates(city)\n",
    "\n",
    "if coordinates:\n",
    "    latitude, longitude = coordinates\n",
    "    print(f\"Coordinates for {city}: Latitude={latitude}, Longitude={longitude}\")\n",
    "else:\n",
    "    print(f\"Coordinates for {city} not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b4482",
   "metadata": {},
   "source": [
    "Answer:6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52010a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4f36190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_gaming_laptops():\n",
    "    url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    laptops = soup.find_all('div', class_='TopNumbeHeading active sticky-footer')\n",
    "    laptop_details = []\n",
    "    for laptop in laptops:\n",
    "        details = {}\n",
    "        details['Name'] = laptop.find('div', class_='heading-wraper').text.strip()\n",
    "        specs = laptop.find_all('div', class_='specs-wrap')\n",
    "        for spec in specs:\n",
    "            spec_name = spec.find('div', class_='Value').text.strip()\n",
    "            spec_value = spec.find('div', class_='heading').text.strip()\n",
    "            details[spec_name] = spec_value\n",
    "        laptop_details.append(details)\n",
    "    return laptop_details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7f60be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops_data = scrape_gaming_laptops()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4a020fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(laptops_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f34afb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25c00d8",
   "metadata": {},
   "source": [
    "Answer:7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49747888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ef2a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_billionaires():\n",
    "    url = \"https://www.forbes.com/billionaires/\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    billionaires = []\n",
    "    \n",
    "    for row in soup.find_all('div', class_='ranked-item'):\n",
    "        rank = row.find('div', class_='rank').text.strip()\n",
    "        name = row.find('div', class_='personName').text.strip()\n",
    "        net_worth = row.find('div', class_='netWorth').text.strip()\n",
    "        age = row.find('div', class_='age').text.strip()\n",
    "        citizenship = row.find('div', class_='countryOfCitizenship').text.strip()\n",
    "        source = row.find('div', class_='source').text.strip()\n",
    "        industry = row.find('div', class_='category').text.strip()\n",
    "        \n",
    "        billionaire = {\n",
    "            \"Rank\": rank,\n",
    "            \"Name\": name,\n",
    "            \"Net worth\": net_worth,\n",
    "            \"Age\": age,\n",
    "            \"Citizenship\": citizenship,\n",
    "            \"Source\": source,\n",
    "            \"Industry\": industry\n",
    "        }\n",
    "        \n",
    "        billionaires.append(billionaire)\n",
    "    \n",
    "    return billionaires\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446cbebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "billionaires_data = scrape_billionaires()\n",
    "df = pd.DataFrame(billionaires_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bad41993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849922d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
